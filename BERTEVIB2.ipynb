{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa95efb4-71eb-4f3d-8799-c12831385900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6b7a75-f6d2-4fd7-968c-b16f9832d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mudanças principais:\n",
    "\n",
    "#Modelo Bertimbau Large: Alterado o model_name para 'neuralmind/bert-large-portuguese-cased'.\n",
    "\n",
    "#LR= 3e-5.\n",
    "\n",
    "#Descongelamento das camadas: Parametrizamos o número de camadas finais do BERT a descongelar, via unfreeze_layers. Por exemplo, se definirmos unfreeze_layers=8, descongelamos as últimas 8 camadas.\n",
    "\n",
    "#Outros otimizadores e LR Schedulers: Mantemos o AdamW como otimizador principal, mas agora adicionamos um scheduler (get_linear_schedule_with_warmup do transformers) para ajustar a taxa de aprendizado durante o treino. Caso queira testar outro otimizador, basta substituir a linha do optimizador. Também deixamos comentado outro exemplo (SGD) para referência.\n",
    "#Para testar diferentes taxas de aprendizado, basta alterar learning_rate no código.\n",
    "#Para testar diferentes números de camadas a descongelar, altere unfreeze_layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e375e916-07a1-44f1-9675-8fb7eb8045f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semente para reprodutibilidade\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b511a6a-4c56-4335-8c15-660d3d146399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configurações gerais\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Usando dispositivo: {device}')\n",
    "\n",
    "# Parametrizações a serem testadas\n",
    "model_name = 'neuralmind/bert-large-portuguese-cased'  # Bertimbau Large\n",
    "learning_rate = 3e-5  # Pode alterar para 5e-5 e comparar\n",
    "unfreeze_layers = 4   # Quantas últimas camadas descongelar? Pode variar (4, 8, 12, etc.)\n",
    "nclasses = 2\n",
    "nepochs = 10\n",
    "batch_size = 16\n",
    "batch_status = 32\n",
    "early_stop = 5\n",
    "max_length = 360\n",
    "write_path = 'modelbB2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a0a71c0-667a-421f-b324-fa857999aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "data = pd.read_csv(\"DATAFRAME.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50f8fd60-955b-45a5-804d-4fcbba2a666a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Treino: 3534\n",
      "Tamanho da Validação: 505\n",
      "Tamanho do Teste: 449\n"
     ]
    }
   ],
   "source": [
    "# Divisão dos dados (ex: 80% treino, 10% val, 10% teste)\n",
    "train_data, test_data = train_test_split(data, test_size=0.10, random_state=seed, stratify=data['contra'])\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.125, random_state=seed, stratify=train_data['contra'])\n",
    "\n",
    "print(f\"Tamanho do Treino: {len(train_data)}\")\n",
    "print(f\"Tamanho da Validação: {len(val_data)}\")\n",
    "print(f\"Tamanho do Teste: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff7f8d91-5750-45f3-9a75-1240e5401757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        label = self.data.iloc[idx]['contra']\n",
    "        inputs = self.tokenizer(text, return_tensors='pt',\n",
    "                                padding='max_length', truncation=True,\n",
    "                                max_length=self.max_length)\n",
    "        return {key: val.squeeze(0) for key, val in inputs.items()}, torch.tensor(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac08faef-e9c9-496f-b850-6295f892c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, model_name, nclasses, unfreeze_layers):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, nclasses)\n",
    "\n",
    "        # Congelar tudo inicialmente\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Descongelar as últimas 'unfreeze_layers' camadas\n",
    "        # A estrutura: self.bert.encoder.layer é uma lista de camadas\n",
    "        # Se unfreeze_layers=4, descongela as últimas 4:\n",
    "        if unfreeze_layers > 0:\n",
    "            for param in self.bert.encoder.layer[-unfreeze_layers:].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        dropped_out = self.dropout(pooled_output)\n",
    "        logits = self.classifier(dropped_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01ce6958-e51e-435d-bf3a-4a2f21a7751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o tokenizador e modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "model = CustomBERTModel(model_name, nclasses, unfreeze_layers).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a01c80e-7cdf-47e9-8c8b-6cb250ae74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otimizador (AdamW)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "#  Exemplo de outro otimizador: optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fbc16ca-d641-4115-8bdc-02d592c42f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de perda (como as classes estão balanceadas, pode usar peso 1:1 ou apenas None)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eac143f-845f-4392-9392-e694b05bb5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets e Dataloaders\n",
    "train_dataset = CustomDataset(train_data, tokenizer, max_length)\n",
    "val_dataset = CustomDataset(val_data, tokenizer, max_length)\n",
    "test_dataset = CustomDataset(test_data, tokenizer, max_length)\n",
    "\n",
    "traindata = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valdata = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "testdata = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c92821b-ef17-4e09-bc63-22541449e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número total de steps para o scheduler\n",
    "total_steps = len(traindata) * nepochs\n",
    "\n",
    "# Scheduler (Linear Warmup and Decay)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=int(0.1 * total_steps), \n",
    "                                            num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29944c32-f333-4903-b5db-2a5c2fd9ca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 [32/221]\tLoss: 0.665173\n",
      "Epoch: 0 [64/221]\tLoss: 0.775455\n",
      "Epoch: 0 [96/221]\tLoss: 0.514712\n",
      "Epoch: 0 [128/221]\tLoss: 0.592443\n",
      "Epoch: 0 [160/221]\tLoss: 0.454059\n",
      "Epoch: 0 [192/221]\tLoss: 0.318480\n",
      "Epoch 0 - Val F1: 0.8555, Val Accuracy: 0.8554\n",
      "Novo melhor modelo salvo.\n",
      "Epoch: 1 [32/221]\tLoss: 0.068220\n",
      "Epoch: 1 [64/221]\tLoss: 0.125038\n",
      "Epoch: 1 [96/221]\tLoss: 0.109355\n",
      "Epoch: 1 [128/221]\tLoss: 0.088465\n",
      "Epoch: 1 [160/221]\tLoss: 0.394401\n",
      "Epoch: 1 [192/221]\tLoss: 0.253314\n",
      "Epoch 1 - Val F1: 0.9343, Val Accuracy: 0.9347\n",
      "Novo melhor modelo salvo.\n",
      "Epoch: 2 [32/221]\tLoss: 0.173910\n",
      "Epoch: 2 [64/221]\tLoss: 0.040854\n",
      "Epoch: 2 [96/221]\tLoss: 0.011813\n",
      "Epoch: 2 [128/221]\tLoss: 0.046809\n",
      "Epoch: 2 [160/221]\tLoss: 0.022446\n",
      "Epoch: 2 [192/221]\tLoss: 0.059119\n",
      "Epoch 2 - Val F1: 0.9841, Val Accuracy: 0.9842\n",
      "Novo melhor modelo salvo.\n",
      "Epoch: 3 [32/221]\tLoss: 0.013219\n",
      "Epoch: 3 [64/221]\tLoss: 0.003791\n",
      "Epoch: 3 [96/221]\tLoss: 0.010020\n",
      "Epoch: 3 [128/221]\tLoss: 0.007323\n",
      "Epoch: 3 [160/221]\tLoss: 0.005662\n",
      "Epoch: 3 [192/221]\tLoss: 0.002374\n",
      "Epoch 3 - Val F1: 0.9861, Val Accuracy: 0.9861\n",
      "Novo melhor modelo salvo.\n",
      "Epoch: 4 [32/221]\tLoss: 0.001938\n",
      "Epoch: 4 [64/221]\tLoss: 0.002355\n",
      "Epoch: 4 [96/221]\tLoss: 0.015127\n",
      "Epoch: 4 [128/221]\tLoss: 0.001611\n",
      "Epoch: 4 [160/221]\tLoss: 0.003945\n",
      "Epoch: 4 [192/221]\tLoss: 0.002034\n",
      "Epoch 4 - Val F1: 0.9822, Val Accuracy: 0.9822\n",
      "Epoch: 5 [32/221]\tLoss: 0.001320\n",
      "Epoch: 5 [64/221]\tLoss: 0.001975\n",
      "Epoch: 5 [96/221]\tLoss: 0.001481\n",
      "Epoch: 5 [128/221]\tLoss: 0.019006\n",
      "Epoch: 5 [160/221]\tLoss: 0.001818\n",
      "Epoch: 5 [192/221]\tLoss: 0.002019\n",
      "Epoch 5 - Val F1: 0.9881, Val Accuracy: 0.9881\n",
      "Novo melhor modelo salvo.\n",
      "Epoch: 6 [32/221]\tLoss: 0.001603\n",
      "Epoch: 6 [64/221]\tLoss: 0.000926\n",
      "Epoch: 6 [96/221]\tLoss: 0.000794\n",
      "Epoch: 6 [128/221]\tLoss: 0.000451\n",
      "Epoch: 6 [160/221]\tLoss: 0.002788\n",
      "Epoch: 6 [192/221]\tLoss: 0.002322\n",
      "Epoch 6 - Val F1: 0.9881, Val Accuracy: 0.9881\n",
      "Novo melhor modelo salvo.\n",
      "Epoch: 7 [32/221]\tLoss: 0.008778\n",
      "Epoch: 7 [64/221]\tLoss: 0.001476\n",
      "Epoch: 7 [96/221]\tLoss: 0.001121\n",
      "Epoch: 7 [128/221]\tLoss: 0.001397\n",
      "Epoch: 7 [160/221]\tLoss: 0.001074\n",
      "Epoch: 7 [192/221]\tLoss: 0.001038\n",
      "Epoch 7 - Val F1: 0.9901, Val Accuracy: 0.9901\n",
      "Novo melhor modelo salvo.\n",
      "Epoch: 8 [32/221]\tLoss: 0.000955\n",
      "Epoch: 8 [64/221]\tLoss: 0.000527\n",
      "Epoch: 8 [96/221]\tLoss: 0.001172\n",
      "Epoch: 8 [128/221]\tLoss: 0.000631\n",
      "Epoch: 8 [160/221]\tLoss: 0.000406\n",
      "Epoch: 8 [192/221]\tLoss: 0.000748\n",
      "Epoch 8 - Val F1: 0.9901, Val Accuracy: 0.9901\n",
      "Epoch: 9 [32/221]\tLoss: 0.001255\n",
      "Epoch: 9 [64/221]\tLoss: 0.000937\n",
      "Epoch: 9 [96/221]\tLoss: 0.000704\n",
      "Epoch: 9 [128/221]\tLoss: 0.001307\n",
      "Epoch: 9 [160/221]\tLoss: 0.000687\n",
      "Epoch: 9 [192/221]\tLoss: 0.001832\n",
      "Epoch 9 - Val F1: 0.9921, Val Accuracy: 0.9921\n",
      "Novo melhor modelo salvo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renat\\AppData\\Local\\Temp\\ipykernel_14356\\2318115068.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(write_path, 'best_model.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desempenho no conjunto de teste:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       235\n",
      "           1       1.00      0.99      0.99       214\n",
      "\n",
      "    accuracy                           0.99       449\n",
      "   macro avg       0.99      0.99      0.99       449\n",
      "weighted avg       0.99      0.99      0.99       449\n",
      "\n",
      "F1 (teste): 0.9933, Accuracy (teste): 0.9933\n"
     ]
    }
   ],
   "source": [
    "# Número total de steps para o scheduler\n",
    "total_steps = len(traindata) * nepochs\n",
    "\n",
    "# Scheduler (Linear Warmup and Decay)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=int(0.1 * total_steps), \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_real, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            logits = model(**inputs)\n",
    "            pred_labels = torch.argmax(logits, 1)\n",
    "\n",
    "            y_real.extend(labels.cpu().tolist())\n",
    "            y_pred.extend(pred_labels.cpu().tolist())\n",
    "\n",
    "    f1 = f1_score(y_real, y_pred, average='weighted')\n",
    "    acc = accuracy_score(y_real, y_pred)\n",
    "    return f1, acc, (y_real, y_pred)\n",
    "\n",
    "if not os.path.exists(write_path):\n",
    "    os.makedirs(write_path)\n",
    "\n",
    "max_f1, repeat = 0, 0\n",
    "for epoch in range(nepochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_idx, (inputs, labels) in enumerate(traindata):\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(**inputs)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Atualiza LR conforme o scheduler\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (batch_idx + 1) % batch_status == 0:\n",
    "            print(f'Epoch: {epoch} [{batch_idx + 1}/{len(traindata)}]\\tLoss: {loss:.6f}')\n",
    "\n",
    "    # Avaliação no conjunto de validação\n",
    "    f1_val, acc_val, _ = evaluate(model, valdata)\n",
    "    print(f'Epoch {epoch} - Val F1: {f1_val:.4f}, Val Accuracy: {acc_val:.4f}')\n",
    "\n",
    "    # Early Stopping baseado no Val F1\n",
    "    if f1_val > max_f1:\n",
    "        torch.save(model.state_dict(), os.path.join(write_path, 'best_model.pth'))\n",
    "        max_f1 = f1_val\n",
    "        repeat = 0\n",
    "        print('Novo melhor modelo salvo.')\n",
    "    else:\n",
    "        repeat += 1\n",
    "\n",
    "    if repeat == early_stop:\n",
    "        print('Early stopping atingido.')\n",
    "        break\n",
    "\n",
    "# Avaliação no conjunto de teste\n",
    "model.load_state_dict(torch.load(os.path.join(write_path, 'best_model.pth')))\n",
    "f1_test, acc_test, (y_real, y_pred) = evaluate(model, testdata)\n",
    "print(\"Desempenho no conjunto de teste:\")\n",
    "print(classification_report(y_real, y_pred, target_names=['0', '1']))\n",
    "print(f\"F1 (teste): {f1_test:.4f}, Accuracy (teste): {acc_test:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
